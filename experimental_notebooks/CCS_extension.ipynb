{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruijiacao/Documents/Ruijia/Research/eliciting_beliefs_llm/eliciting_beliefs_ai/.conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import utils\n",
    "import torch\n",
    "import numpy as np\n",
    "import all_classes\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForMaskedLM, AutoModelForCausalLM, AutoModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load datasets\n",
    "dataset_name = \"imdb\"\n",
    "# dataset_name = \"amazon_polarity\"\n",
    "data_imdb_train = load_dataset(\"imdb\")[\"train\"]\n",
    "data_imdb_train = np.array(data_imdb_train)\n",
    "data_imdb_test = load_dataset(\"imdb\")[\"test\"]\n",
    "data_imdb_test = np.array(data_imdb_test)\n",
    "# data_testing = load_dataset(\"amazon_polarity\")[\"test\"]\n",
    "# data_testing = load_dataset(\"domenicrosati/TruthfulQA\")[\"train\"]\n",
    "data_boolq_test = load_dataset(\"google/boolq\")[\"validation\"]\n",
    "data_boolq_test = np.array(data_boolq_test)\n",
    "# data = load_dataset(\"amazon_polarity\")[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are a few different model options you can play around with:\n",
    "model_name = \"llama3.2\"\n",
    "# model_name = \"llama3.3\"\n",
    "\n",
    "# the number of hidden dimensions of the model\n",
    "hidden_size = 2048\n",
    "num_layers = 16\n",
    "\n",
    "cache_dir = None\n",
    "\n",
    "if model_name == \"llama3.2\":\n",
    "    model_type = \"decoder\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", cache_dir=cache_dir, token=True)\n",
    "    model = AutoModel.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", cache_dir=cache_dir, token=True)\n",
    "    hidden_size = model.config.hidden_size\n",
    "    num_layers = model.config.num_hidden_layers\n",
    "else:\n",
    "    print(\"Not implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'can you have too much oxygen in your body', 'answer': True, 'passage': 'The result of breathing increased partial pressures of oxygen is hyperoxia, an excess of oxygen in body tissues. The body is affected in different ways depending on the type of exposure. Central nervous system toxicity is caused by short exposure to high partial pressures of oxygen at greater than atmospheric pressure. Pulmonary and ocular toxicity result from longer exposure to increased oxygen levels at normal pressure. Symptoms may include disorientation, breathing problems, and vision changes such as myopia. Prolonged exposure to above-normal oxygen partial pressures, or shorter exposures to very high partial pressures, can cause oxidative damage to cell membranes, collapse of the alveoli in the lungs, retinal detachment, and seizures. Oxygen toxicity is managed by reducing the exposure to increased oxygen levels. Studies show that, in the long term, a robust recovery from most types of oxygen toxicity is possible.'}\n"
     ]
    }
   ],
   "source": [
    "print(data_boolq_test[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Performance of Probes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions for Formatting Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_imdb(text, label, prompt_version = 1):\n",
    "    \"\"\"\n",
    "    Given an imdb example (\"text\") and corresponding label (0 for negative, or 1 for positive),\n",
    "    returns a zero-shot prompt for that example (which includes that label as the answer).\n",
    "\n",
    "    (This is just one example of a simple, manually created prompt.)\n",
    "    \"\"\"\n",
    "    return \"Consider the sentiment of the following review:\\n\" + text + \"\\nDoes the above movie review express a \" + [\"negative\", \"positive\"][label] + \" sentiment? \" + \"Answer: \" + \"Yes\"\n",
    "\n",
    "def format_boolq(text, question, label):\n",
    "    \"\"\"\n",
    "    Given a BoolQA example (\"text\") with the corresponding question and label (1 for \"Yes\" and 0 for \"No\"),\n",
    "    returns a zero-shot prompt for that example (which includes that label as the answer).\n",
    "\n",
    "    (This is just one example of a simple, manually created prompt.)\n",
    "    \"\"\"\n",
    "    return \"Consider the following passage:\\n\" + text + \"\\n\" + \"After reading this passage, I have a question: \" + question + \"?\" + \" True or False?\" + \" Answer: \" + [\"True\", \"False\"][label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, var_normalize = False):\n",
    "    \"\"\"\n",
    "    Mean-normalizes the data x (of shape (n, d))\n",
    "    If self.var_normalize, also divides by the standard deviation\n",
    "    \"\"\"\n",
    "    normalized_x = x - x.mean(axis=0, keepdims=True)\n",
    "    if var_normalize:\n",
    "        normalized_x /= normalized_x.std(axis=0, keepdims=True)\n",
    "\n",
    "    return normalized_x\n",
    "\n",
    "def get_credence(probe, x0_test, x1_test, device = \"cpu\"):\n",
    "    \"\"\"\n",
    "    Given a probe, compute credence for the current parameters on the given test inputs\n",
    "    \"\"\"\n",
    "    x0 = torch.tensor(normalize(x0_test), dtype=torch.float, requires_grad=False, device=device)\n",
    "    x1 = torch.tensor(normalize(x1_test), dtype=torch.float, requires_grad=False, device=device)\n",
    "    with torch.no_grad():\n",
    "        p0, p1 = probe(x0), probe(x1)\n",
    "    avg_confidence = 0.5 * (p0 + (1 - p1))\n",
    "\n",
    "    return avg_confidence\n",
    "\n",
    "def get_random_samples(data_set, n):\n",
    "    return np.random.choice(data_set, n)\n",
    "\n",
    "\n",
    "def get_hidden_states_many_examples(model, tokenizer, data, dataset_name, model_type, params):\n",
    "    \"\"\"\n",
    "    Given an encoder-decoder model, a list of data, computes the contrast hidden states on n random examples by probing the model according to the specified parameters (param = (layer_indices, token_positions, prompt_versions))\n",
    "    Returns numpy arrays of shape (n, hidden_dim) for each candidate label, along with a boolean numpy array of shape (n,)\n",
    "    with the ground truth labels\n",
    "    \n",
    "    This is deliberately simple so that it's easy to understand, rather than being optimized for efficiency\n",
    "    \"\"\"\n",
    "    # setup\n",
    "    model.eval()\n",
    "    all_neg_hs, all_pos_hs, all_gt_labels = [], [], []\n",
    "    layer, token_pos, prompt_version = params \n",
    "\n",
    "    # loop\n",
    "    for sample in data:\n",
    "        if dataset_name == \"imdb\":\n",
    "            text, true_label = sample[\"text\"], sample[\"label\"]\n",
    "            neg_hs = utils.get_hidden_states(model, tokenizer, format_imdb(text, 0, prompt_version), token_pos, layer, model_type=model_type)\n",
    "            pos_hs = utils.get_hidden_states(model, tokenizer, format_imdb(text, 1, prompt_version), token_pos, layer, model_type=model_type)\n",
    "        elif dataset_name == \"boolq\":\n",
    "            text, question, true_label = sample[\"passage\"], sample[\"question\"], sample[\"answer\"]\n",
    "            neg_hs = utils.get_hidden_states(model, tokenizer, format_boolq(text, question, 0), token_pos, layer, model_type=model_type)\n",
    "            pos_hs = utils.get_hidden_states(model, tokenizer, format_boolq(text, question, 1), token_pos, layer, model_type=model_type)\n",
    "\n",
    "        # collect\n",
    "        all_neg_hs.append(neg_hs)\n",
    "        all_pos_hs.append(pos_hs)\n",
    "        all_gt_labels.append(true_label)\n",
    "\n",
    "    all_neg_hs = np.stack(all_neg_hs)\n",
    "    all_pos_hs = np.stack(all_pos_hs)\n",
    "    all_gt_labels = np.stack(all_gt_labels)\n",
    "\n",
    "    return all_neg_hs, all_pos_hs, all_gt_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data_testing[0])\n",
    "# text, question, answer = data_testing[0][\"passage\"], data_testing[0][\"question\"], data_testing[0][\"answer\"]\n",
    "# print(format_boolq(text, question, answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions for Aggregating Credences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate a list of credences into one estimate using geometric mean\n",
    "def aggregate_gmean(credences):\n",
    "    k = np.shape(credences)[0]\n",
    "    result = np.power(np.prod(credences * (1 / (1 - credences))), 1 / k)\n",
    "    return 1 / (1 + result)\n",
    "\n",
    "# (To-DO) aggregragate a list of credences using weighted geometric mean\n",
    "def aggregate_gmean_weighted(credences, weights):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing a single probe on a different dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Constructing CCS Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "num_example = 100\n",
    "# layer_idices = [1, 3, 5, 7, -1] \n",
    "layer_indices = [-1]\n",
    "# token_positions = [-2, -1]\n",
    "token_positions = [-1]\n",
    "prompt_versions = [1] \n",
    "imdb_train_sample = get_random_samples(data_imdb_train, num_example) \n",
    "imdb_test_sample = get_random_samples(data_imdb_test, num_example)\n",
    "boolq_test_sample = get_random_samples(data_boolq_test, num_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states_all_imdb_train = {}\n",
    "for (token_pos, layer_idx, version) in itertools.product(token_positions, layer_indices, prompt_versions):\n",
    "    neg_hs, pos_hs, y = get_hidden_states_many_examples(model, tokenizer, imdb_train_sample, \"imdb\", model_type, (token_pos, layer_idx, version))\n",
    "    hidden_states_all_imdb_train[(token_pos, layer_idx, version)] = (neg_hs, pos_hs, y)\n",
    "\n",
    "# neg_hs, pos_hs, y = get_hidden_states_many_examples(model, tokenizer, data_sampled, dataset_name, model_type, (-1, -1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states_all_boolq_test = {}\n",
    "for (token_pos, layer_idx, version) in itertools.product(token_positions, layer_indices, prompt_versions):\n",
    "    neg_hs, pos_hs, y = get_hidden_states_many_examples(model, tokenizer, boolq_test_sample, \"boolq\", model_type, (token_pos, layer_idx, version))\n",
    "    hidden_states_all_boolq_test[(token_pos, layer_idx, version)] = (neg_hs, pos_hs, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states_all_imdb_test = {}\n",
    "for (token_pos, layer_idx, version) in itertools.product(token_positions, layer_indices, prompt_versions):\n",
    "    neg_hs, pos_hs, y = get_hidden_states_many_examples(model, tokenizer, imdb_test_sample, \"imdb\", model_type, (token_pos, layer_idx, version))\n",
    "    hidden_states_all_imdb_test[(token_pos, layer_idx, version)] = (neg_hs, pos_hs, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(hidden_states_all[(-1, -1, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "probes = {} # a python dictionary of best probes accordng to the specified testing hyperparameters\n",
    "probabilities_imdb = {} # a python dictionary of credences for imdb \n",
    "probabilities_boolq = {} # a python dictionary of credences for boolq \n",
    "log_reg = {}\n",
    "for (token_pos, layer_idx, version) in itertools.product(token_positions, layer_indices, prompt_versions):\n",
    "    neg_hs_train, pos_hs_train, y_train = hidden_states_all_imdb_train[(token_pos, layer_idx, version)]\n",
    "    neg_hs_test, pos_hs_test, y_test = hidden_states_all_boolq_test[(token_pos, layer_idx, version)]\n",
    "    neg_hs_test_imdb, pos_hs_test_imdb, y_test_imdb = hidden_states_all_imdb_test[(token_pos, layer_idx, version)]\n",
    "    credence_estimator = all_classes.CE(neg_hs_train, pos_hs_train)\n",
    "    credence_estimator.repeated_train()\n",
    "    cur_best_probe = credence_estimator.get_best_probe()\n",
    "    probes[(token_pos, layer_idx, version)] = cur_best_probe\n",
    "    probabilities_imdb[(token_pos, layer_idx, version)] = credence_estimator.get_credence(neg_hs_test_imdb, pos_hs_test_imdb).detach().cpu().numpy()\n",
    "    probabilities_boolq[(token_pos, layer_idx, version)] = credence_estimator.get_credence(neg_hs_test, pos_hs_test).detach().cpu().numpy()\n",
    "    # compute logistic regression\n",
    "    # x_train = neg_hs_train - pos_hs_train\n",
    "    # x_test = neg_hs_test - pos_hs_test\n",
    "\n",
    "    # lr = LogisticRegression(class_weight=\"balanced\")\n",
    "    # lr.fit(x_train, y_train)\n",
    "    # log_reg[(token_pos, layer_idx, version)] = lr.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing Accuracy of a Single Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8402919  0.6539956  0.57700455 0.6361449  0.6501466  0.44543847\n",
      " 0.45764908 0.7188328  0.6441877  0.5461449  0.57696974 0.39202917\n",
      " 0.19695526 0.43606633 0.54486644 0.37412882 0.39373714 0.64184487\n",
      " 0.36837864 0.50485766 0.5897001  0.5559048  0.42092907 0.46217716\n",
      " 0.4951108  0.29977977 0.4811827  0.49284863 0.39753056 0.6223022\n",
      " 0.35655862 0.5015032  0.49546957 0.5075362  0.34753865 0.46260408\n",
      " 0.6956546  0.56233734 0.4221326  0.07767093 0.5613854  0.3007778\n",
      " 0.8726037  0.15887421 0.5042697  0.48101798 0.5064451  0.43705216\n",
      " 0.42771053 0.6488231  0.650629   0.3076182  0.5332646  0.43858504\n",
      " 0.55537087 0.45665193 0.47974932 0.5068742  0.4523391  0.5837478\n",
      " 0.5377515  0.5673733  0.49115518 0.54270136 0.44724166 0.5041692\n",
      " 0.5698343  0.5877314  0.35894692 0.541916   0.5221902  0.3947582\n",
      " 0.23734176 0.53131324 0.45639682 0.4453076  0.7024766  0.5653941\n",
      " 0.36931908 0.59759176 0.58428955 0.51664454 0.81606424 0.47467557\n",
      " 0.21465065 0.49270272 0.66228    0.43390802 0.5946769  0.3410297\n",
      " 0.7626904  0.45764908 0.6182726  0.45074975 0.47924262 0.636165\n",
      " 0.5208859  0.35552508 0.60105497 0.57680964]\n",
      "0.52\n",
      "0.52\n"
     ]
    }
   ],
   "source": [
    "_, _, y_test_boolq = hidden_states_all_boolq_test[(-1, -1, 1)]\n",
    "_, _, y_test_imdb = hidden_states_all_imdb_test[(-1, -1, 1)]\n",
    "credences_aggregated_boolq = []\n",
    "credences_aggregated_imdb = []\n",
    "for i in range(num_example):\n",
    "    # list of all credences of example i\n",
    "    all_estimates_imdb = [credences[i] for credences in probabilities_imdb.values()]\n",
    "    all_estimates_boolq = [credences[i] for credences in probabilities_boolq.values()]\n",
    "    credences_aggregated_boolq.append(aggregate_gmean(np.array(all_estimates_imdb)))\n",
    "    credences_aggregated_imdb.append(aggregate_gmean(np.array(all_estimates_boolq)))\n",
    "credences_aggregated_imdb = np.array(credences_aggregated_imdb)\n",
    "credences_aggregated_boolq = np.array(credences_aggregated_boolq)\n",
    "# print(credences_aggregated_boolq)\n",
    "print(credences_aggregated_imdb)\n",
    "# y_test = all_y[num_example // 2 : ]\n",
    "predictions_boolq = (credences_aggregated_boolq < 0.5).astype(int)\n",
    "acc_boolq = (predictions_boolq == y_test_boolq).mean()\n",
    "acc_boolq = max(acc_boolq, 1 - acc_boolq)\n",
    "print(acc_boolq)\n",
    "\n",
    "predictions_imdb = (credences_aggregated_imdb < 0.5).astype(int)\n",
    "acc_imdb = (predictions_imdb == y_test_imdb).mean()\n",
    "acc_imdb = max(acc_imdb, 1 - acc_imdb)\n",
    "print(acc_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False  True False  True  True  True  True False  True  True False\n",
      "  True False False False False  True False False  True  True  True False\n",
      " False  True  True  True  True  True False  True False  True  True False\n",
      "  True  True  True  True  True  True  True False  True False  True  True\n",
      "  True  True  True False False  True  True False False False  True False\n",
      "  True False False  True  True  True  True  True  True  True  True  True\n",
      "  True  True False False False False False False  True  True  True False\n",
      " False  True  True False  True  True  True  True  True False False False\n",
      "  True  True  True  True]\n",
      "[0 0 1 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 0 1 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0\n",
      " 1 1 0 1 1 1 1 0 0 0 0 1 0 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 1 1 0 1 1 0 1 0 0\n",
      " 1 1 0 0 1 1 1 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_test_boolq)\n",
    "print(y_test_imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing Accuracies of the Probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(-2, 1, 'imdb_1')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     credences_aggregated\u001b[38;5;241m.\u001b[39mappend(aggregate_gmean(np\u001b[38;5;241m.\u001b[39marray(all_estimates)))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# list of true labels from the test set (the training examples are the same in training the probes)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m _, _, all_y \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states_all\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimdb_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     10\u001b[0m credences_aggregated \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(credences_aggregated)\n\u001b[1;32m     11\u001b[0m y_test \u001b[38;5;241m=\u001b[39m all_y[num_example \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m : ]\n",
      "\u001b[0;31mKeyError\u001b[0m: (-2, 1, 'imdb_1')"
     ]
    }
   ],
   "source": [
    "# list of aggregated credences\n",
    "credences_aggregated = []\n",
    "for i in range(num_example // 2):\n",
    "    # list of all credences of example i\n",
    "    all_estimates = [credences[i] for credences in probabilities.values()]\n",
    "    credences_aggregated.append(aggregate_gmean(np.array(all_estimates)))\n",
    "\n",
    "# list of true labels from the test set (the training examples are the same in training the probes)\n",
    "_, _, y_test = hidden_states_all_testing[(-1, -1, 1)]\n",
    "credences_aggregated = np.array(credences_aggregated)\n",
    "# y_test = all_y[num_example // 2 : ]\n",
    "predictions = (credences_aggregated < 0.5).astype(int)\n",
    "acc = (predictions == y_test).mean()\n",
    "acc = max(acc, 1 - acc)\n",
    "print(acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
