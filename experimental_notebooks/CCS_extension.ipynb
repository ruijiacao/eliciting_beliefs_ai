{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import utils\n",
    "import torch\n",
    "import numpy as np\n",
    "import all_classes\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForMaskedLM, AutoModelForCausalLM, AutoModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load datasets\n",
    "dataset_name = \"imdb\"\n",
    "# dataset_name = \"amazon_polarity\"\n",
    "data = load_dataset(\"imdb\")[\"test\"]\n",
    "data = np.array(data)\n",
    "# data_testing = load_dataset(\"amazon_polarity\")[\"test\"]\n",
    "# data_testing = load_dataset(\"domenicrosati/TruthfulQA\")[\"train\"]\n",
    "data_testing = load_dataset(\"google/boolq\")[\"validation\"]\n",
    "data_testing = np.array(data_testing)\n",
    "# data = load_dataset(\"amazon_polarity\")[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are a few different model options you can play around with:\n",
    "model_name = \"llama3.2\"\n",
    "# model_name = \"llama3.3\"\n",
    "\n",
    "# the number of hidden dimensions of the model\n",
    "hidden_size = 2048\n",
    "num_layers = 16\n",
    "\n",
    "cache_dir = None\n",
    "\n",
    "if model_name == \"llama3.2\":\n",
    "    model_type = \"decoder\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", cache_dir=cache_dir, token=True)\n",
    "    model = AutoModel.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", cache_dir=cache_dir, token=True)\n",
    "    hidden_size = model.config.hidden_size\n",
    "    num_layers = model.config.num_hidden_layers\n",
    "else:\n",
    "    print(\"Not implemented!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'does ethanol take more energy make that produces', 'answer': False, 'passage': \"All biomass goes through at least some of these steps: it needs to be grown, collected, dried, fermented, distilled, and burned. All of these steps require resources and an infrastructure. The total amount of energy input into the process compared to the energy released by burning the resulting ethanol fuel is known as the energy balance (or ``energy returned on energy invested''). Figures compiled in a 2007 report by National Geographic Magazine point to modest results for corn ethanol produced in the US: one unit of fossil-fuel energy is required to create 1.3 energy units from the resulting ethanol. The energy balance for sugarcane ethanol produced in Brazil is more favorable, with one unit of fossil-fuel energy required to create 8 from the ethanol. Energy balance estimates are not easily produced, thus numerous such reports have been generated that are contradictory. For instance, a separate survey reports that production of ethanol from sugarcane, which requires a tropical climate to grow productively, returns from 8 to 9 units of energy for each unit expended, as compared to corn, which only returns about 1.34 units of fuel energy for each unit of energy expended. A 2006 University of California Berkeley study, after analyzing six separate studies, concluded that producing ethanol from corn uses much less petroleum than producing gasoline.\"}\n"
     ]
    }
   ],
   "source": [
    "print(data_testing[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving Performance of Probes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions for Formatting Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_imdb(text, label, prompt_version = 1):\n",
    "    \"\"\"\n",
    "    Given an imdb example (\"text\") and corresponding label (0 for negative, or 1 for positive),\n",
    "    returns a zero-shot prompt for that example (which includes that label as the answer).\n",
    "\n",
    "    (This is just one example of a simple, manually created prompt.)\n",
    "    \"\"\"\n",
    "    return \"Consider the sentiment of the following review:\\n\" + text + \"\\nDoes the above movie review express a \" + [\"negative\", \"positive\"][label] + \" sentiment? \" + \"Answer: \" + \"Yes\"\n",
    "\n",
    "def format_boolq(text, question, label):\n",
    "    \"\"\"\n",
    "    Given a BoolQA example (\"text\") with the corresponding question and label (1 for \"Yes\" and 0 for \"No\"),\n",
    "    returns a zero-shot prompt for that example (which includes that label as the answer).\n",
    "\n",
    "    (This is just one example of a simple, manually created prompt.)\n",
    "    \"\"\"\n",
    "    return \"Consider the following passage:\\n\" + text + \"\\n\" + \"After reading this passage, I have a question: \" + question + \"?\" + \" True or False?\" + \" Answer: \" + [\"True\", \"False\"][label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, var_normalize = False):\n",
    "    \"\"\"\n",
    "    Mean-normalizes the data x (of shape (n, d))\n",
    "    If self.var_normalize, also divides by the standard deviation\n",
    "    \"\"\"\n",
    "    normalized_x = x - x.mean(axis=0, keepdims=True)\n",
    "    if var_normalize:\n",
    "        normalized_x /= normalized_x.std(axis=0, keepdims=True)\n",
    "\n",
    "    return normalized_x\n",
    "\n",
    "def get_credence(probe, x0_test, x1_test, device = \"cpu\"):\n",
    "    \"\"\"\n",
    "    Given a probe, compute credence for the current parameters on the given test inputs\n",
    "    \"\"\"\n",
    "    x0 = torch.tensor(normalize(x0_test), dtype=torch.float, requires_grad=False, device=device)\n",
    "    x1 = torch.tensor(normalize(x1_test), dtype=torch.float, requires_grad=False, device=device)\n",
    "    with torch.no_grad():\n",
    "        p0, p1 = probe(x0), probe(x1)\n",
    "    avg_confidence = 0.5 * (p0 + (1 - p1))\n",
    "\n",
    "    return avg_confidence\n",
    "\n",
    "def get_random_samples(data_set, n):\n",
    "    return np.random.choice(data_set, n)\n",
    "\n",
    "\n",
    "def get_hidden_states_many_examples(model, tokenizer, data, dataset_name, model_type, params):\n",
    "    \"\"\"\n",
    "    Given an encoder-decoder model, a list of data, computes the contrast hidden states on n random examples by probing the model according to the specified parameters (param = (layer_indices, token_positions, prompt_versions))\n",
    "    Returns numpy arrays of shape (n, hidden_dim) for each candidate label, along with a boolean numpy array of shape (n,)\n",
    "    with the ground truth labels\n",
    "    \n",
    "    This is deliberately simple so that it's easy to understand, rather than being optimized for efficiency\n",
    "    \"\"\"\n",
    "    # setup\n",
    "    model.eval()\n",
    "    all_neg_hs, all_pos_hs, all_gt_labels = [], [], []\n",
    "    layer, token_pos, prompt_version = params \n",
    "\n",
    "    # loop\n",
    "    for sample in data:\n",
    "        if dataset_name == \"imdb\":\n",
    "            text, true_label = sample[\"text\"], sample[\"label\"]\n",
    "            neg_hs = utils.get_hidden_states(model, tokenizer, format_imdb(text, 0, prompt_version), token_pos, layer, model_type=model_type)\n",
    "            pos_hs = utils.get_hidden_states(model, tokenizer, format_imdb(text, 1, prompt_version), token_pos, layer, model_type=model_type)\n",
    "        elif dataset_name == \"boolq\":\n",
    "            text, question, true_label = sample[\"passage\"], sample[\"question\"], sample[\"answer\"]\n",
    "            neg_hs = utils.get_hidden_states(model, tokenizer, format_boolq(text, question, 0), token_pos, layer, model_type=model_type)\n",
    "            pos_hs = utils.get_hidden_states(model, tokenizer, format_boolq(text, question, 1), token_pos, layer, model_type=model_type)\n",
    "\n",
    "        # collect\n",
    "        all_neg_hs.append(neg_hs)\n",
    "        all_pos_hs.append(pos_hs)\n",
    "        all_gt_labels.append(true_label)\n",
    "\n",
    "    all_neg_hs = np.stack(all_neg_hs)\n",
    "    all_pos_hs = np.stack(all_pos_hs)\n",
    "    all_gt_labels = np.stack(all_gt_labels)\n",
    "\n",
    "    return all_neg_hs, all_pos_hs, all_gt_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for Formatting Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_imdb(text, label):\n",
    "    \"\"\"\n",
    "    Given an imdb example (\"text\") and corresponding label (0 for negative, or 1 for positive),\n",
    "    returns a zero-shot prompt for that example (which includes that label as the answer).\n",
    "\n",
    "    (This is just one example of a simple, manually created prompt.)\n",
    "    \"\"\"\n",
    "    return \"Consider the sentiment of the following review:\\n\" + text + \"\\nDoes the above movie review express a \" + [\"negative\", \"positive\"][label] + \" sentiment? \" + \"Answer: \" + \"Yes\"\n",
    "\n",
    "def format_boolqa(text, question, label):\n",
    "    \"\"\"\n",
    "    Given a BoolQA example (\"text\") with the corresponding question and label (1 for \"Yes\" and 0 for \"No\"),\n",
    "    returns a zero-shot prompt for that example (which includes that label as the answer).\n",
    "\n",
    "    (This is just one example of a simple, manually created prompt.)\n",
    "    \"\"\"\n",
    "    return \"Consider the following passage:\\n\" + text + \"\\n\" + \"After reading this passage, I have a question: \" + question + \"?\" + \" True or False?\" + \" Answer: \" + [\"True\", \"False\"][label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'does ethanol take more energy make that produces', 'answer': False, 'passage': \"All biomass goes through at least some of these steps: it needs to be grown, collected, dried, fermented, distilled, and burned. All of these steps require resources and an infrastructure. The total amount of energy input into the process compared to the energy released by burning the resulting ethanol fuel is known as the energy balance (or ``energy returned on energy invested''). Figures compiled in a 2007 report by National Geographic Magazine point to modest results for corn ethanol produced in the US: one unit of fossil-fuel energy is required to create 1.3 energy units from the resulting ethanol. The energy balance for sugarcane ethanol produced in Brazil is more favorable, with one unit of fossil-fuel energy required to create 8 from the ethanol. Energy balance estimates are not easily produced, thus numerous such reports have been generated that are contradictory. For instance, a separate survey reports that production of ethanol from sugarcane, which requires a tropical climate to grow productively, returns from 8 to 9 units of energy for each unit expended, as compared to corn, which only returns about 1.34 units of fuel energy for each unit of energy expended. A 2006 University of California Berkeley study, after analyzing six separate studies, concluded that producing ethanol from corn uses much less petroleum than producing gasoline.\"}\n",
      "Consider the following passage:\n",
      "All biomass goes through at least some of these steps: it needs to be grown, collected, dried, fermented, distilled, and burned. All of these steps require resources and an infrastructure. The total amount of energy input into the process compared to the energy released by burning the resulting ethanol fuel is known as the energy balance (or ``energy returned on energy invested''). Figures compiled in a 2007 report by National Geographic Magazine point to modest results for corn ethanol produced in the US: one unit of fossil-fuel energy is required to create 1.3 energy units from the resulting ethanol. The energy balance for sugarcane ethanol produced in Brazil is more favorable, with one unit of fossil-fuel energy required to create 8 from the ethanol. Energy balance estimates are not easily produced, thus numerous such reports have been generated that are contradictory. For instance, a separate survey reports that production of ethanol from sugarcane, which requires a tropical climate to grow productively, returns from 8 to 9 units of energy for each unit expended, as compared to corn, which only returns about 1.34 units of fuel energy for each unit of energy expended. A 2006 University of California Berkeley study, after analyzing six separate studies, concluded that producing ethanol from corn uses much less petroleum than producing gasoline.\n",
      "After reading this passage, I have a question: does ethanol take more energy make that produces? True or False? Answer: True\n"
     ]
    }
   ],
   "source": [
    "print(data_testing[0])\n",
    "text, question, answer = data_testing[0][\"passage\"], data_testing[0][\"question\"], data_testing[0][\"answer\"]\n",
    "print(format_boolqa(text, question, answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Functions for Aggregating Credences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate a list of credences into one estimate using geometric mean\n",
    "def aggregate_gmean(credences):\n",
    "    k = np.shape(credences)[0]\n",
    "    result = np.power(np.prod(credences * (1 / (1 - credences))), 1 / k)\n",
    "    return 1 / (1 + result)\n",
    "\n",
    "# (To-DO) aggregragate a list of credences using weighted geometric mean\n",
    "def aggregate_gmean_weighted(credences, weights):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing a single probe on a different dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Constructing CCS Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "num_example = 100\n",
    "# layer_idices = [1, 3, 5, 7, -1] \n",
    "layer_indices = [-1]\n",
    "# token_positions = [-2, -1]\n",
    "token_positions = [-1]\n",
    "prompt_versions = [1] \n",
    "data_sampled = get_random_samples(data, num_example)\n",
    "data_sampled_testing = get_random_samples(data_testing, num_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states_all = {}\n",
    "for (token_pos, layer_idx, version) in itertools.product(token_positions, layer_indices, prompt_versions):\n",
    "    neg_hs, pos_hs, y = get_hidden_states_many_examples(model, tokenizer, data_sampled, dataset_name, model_type, (token_pos, layer_idx, version))\n",
    "    hidden_states_all[(token_pos, layer_idx, version)] = (neg_hs, pos_hs, y)\n",
    "\n",
    "# neg_hs, pos_hs, y = get_hidden_states_many_examples(model, tokenizer, data_sampled, dataset_name, model_type, (-1, -1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states_all_testing = {}\n",
    "for (token_pos, layer_idx, version) in itertools.product(token_positions, layer_indices, prompt_versions):\n",
    "    neg_hs, pos_hs, y = get_hidden_states_many_examples(model, tokenizer, data_sampled_testing, \"boolq\", model_type, (token_pos, layer_idx, version))\n",
    "    hidden_states_all_testing[(token_pos, layer_idx, version)] = (neg_hs, pos_hs, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ 0.04327318,  0.31138775,  3.0550823 , ...,  0.04004868,\n",
      "         3.7564766 , -1.8230091 ],\n",
      "       [-0.7510771 ,  0.90223783,  2.7056472 , ...,  1.541798  ,\n",
      "         3.5128944 ,  1.4865934 ],\n",
      "       [-0.6196999 ,  0.27405262,  2.9780238 , ...,  0.72248644,\n",
      "         3.4120562 , -1.2429531 ],\n",
      "       ...,\n",
      "       [-1.4143257 ,  0.33925962,  3.459448  , ...,  1.6323417 ,\n",
      "         3.7051625 , -0.52412343],\n",
      "       [-0.7334767 ,  1.6315787 ,  3.8417377 , ...,  0.9043974 ,\n",
      "         2.7949839 ,  0.23785806],\n",
      "       [-0.99741   ,  0.31599328,  3.4712873 , ...,  1.2866738 ,\n",
      "         4.4964705 , -0.12033664]], shape=(100, 2048), dtype=float32), array([[-0.97678214,  1.6671463 ,  2.8702164 , ...,  1.7272348 ,\n",
      "         3.0499668 , -1.489957  ],\n",
      "       [-0.531275  ,  1.0922753 ,  3.3301604 , ...,  0.3380359 ,\n",
      "         4.8146496 ,  0.07132591],\n",
      "       [-1.5116814 ,  1.4539078 ,  2.3280592 , ...,  1.8367846 ,\n",
      "         3.756009  , -0.930765  ],\n",
      "       ...,\n",
      "       [-1.109737  ,  1.1791164 ,  3.8098383 , ...,  2.2681398 ,\n",
      "         4.576609  , -0.99581426],\n",
      "       [-0.77759385,  2.0222046 ,  4.024689  , ...,  0.5781163 ,\n",
      "         3.4311833 , -0.03432916],\n",
      "       [-0.9189466 ,  1.0669059 ,  3.4883592 , ...,  1.9872112 ,\n",
      "         5.6500516 , -0.50303286]], shape=(100, 2048), dtype=float32), array([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
      "       1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1,\n",
      "       1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,\n",
      "       0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0,\n",
      "       1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0]))\n"
     ]
    }
   ],
   "source": [
    "print(hidden_states_all[(-1, -1, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "probes = {} # a python dictionary of best probes accordng to the specified testing hyperparameters\n",
    "probabilities = {} # a python dictionary of credences according to \n",
    "log_reg = {}\n",
    "for (token_pos, layer_idx, version) in itertools.product(token_positions, layer_indices, prompt_versions):\n",
    "    neg_hs_train, pos_hs_train, y_train = hidden_states_all[(token_pos, layer_idx, version)]\n",
    "    neg_hs_test, pos_hs_test, y_test = hidden_states_all[(token_pos, layer_idx, version)]\n",
    "    credence_estimator = all_classes.CE(neg_hs_train, pos_hs_train)\n",
    "    credence_estimator.repeated_train()\n",
    "    cur_best_probe = credence_estimator.get_best_probe()\n",
    "    probes[(token_pos, layer_idx, version)] = cur_best_probe\n",
    "    probabilities[(token_pos, layer_idx, version)] = credence_estimator.get_credence(neg_hs_test, pos_hs_test).detach().cpu().numpy()\n",
    "    # compute logistic regression\n",
    "    # x_train = neg_hs_train - pos_hs_train\n",
    "    # x_test = neg_hs_test - pos_hs_test\n",
    "\n",
    "    # lr = LogisticRegression(class_weight=\"balanced\")\n",
    "    # lr.fit(x_train, y_train)\n",
    "    # log_reg[(token_pos, layer_idx, version)] = lr.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing Accuracies of the Probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(-2, 1, 'imdb_1')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     credences_aggregated\u001b[38;5;241m.\u001b[39mappend(aggregate_gmean(np\u001b[38;5;241m.\u001b[39marray(all_estimates)))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# list of true labels from the test set (the training examples are the same in training the probes)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m _, _, all_y \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states_all\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimdb_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     10\u001b[0m credences_aggregated \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(credences_aggregated)\n\u001b[1;32m     11\u001b[0m y_test \u001b[38;5;241m=\u001b[39m all_y[num_example \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m : ]\n",
      "\u001b[0;31mKeyError\u001b[0m: (-2, 1, 'imdb_1')"
     ]
    }
   ],
   "source": [
    "# list of aggregated credences\n",
    "credences_aggregated = []\n",
    "for i in range(num_example // 2):\n",
    "    # list of all credences of example i\n",
    "    all_estimates = [credences[i] for credences in probabilities.values()]\n",
    "    credences_aggregated.append(aggregate_gmean(np.array(all_estimates)))\n",
    "\n",
    "# list of true labels from the test set (the training examples are the same in training the probes)\n",
    "_, _, all_y = hidden_states_all[(-2, 1, \"imdb_1\")]\n",
    "credences_aggregated = np.array(credences_aggregated)\n",
    "y_test = all_y[num_example // 2 : ]\n",
    "predictions = (credences_aggregated < 0.5).astype(int)\n",
    "acc = (predictions == y_test).mean()\n",
    "acc = max(acc, 1 - acc)\n",
    "print(acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
