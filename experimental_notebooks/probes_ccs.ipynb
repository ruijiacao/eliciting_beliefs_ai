{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Probes using Contrast-Consistent Search (CCS)\n",
    "\n",
    "This notebook demonstrates how to train probes using the CCS approach from Burns et al. (2023). CCS is a method for learning probes that can extract beliefs from language models without using labeled data.\n",
    "\n",
    "The key idea is to train a probe that:\n",
    "1. Makes informative predictions (assigns high probabilities to at least one of the contrast pairs)\n",
    "2. Makes consistent predictions (assigns complementary probabilities to contrast pairs)\n",
    "\n",
    "For example, if we have a statement \"The sky is blue\", we create two contrast pairs:\n",
    "- \"The sky is blue\" (positive)\n",
    "- \"The sky is not blue\" (negative)\n",
    "\n",
    "The probe should assign high probability to one of these and low probability to the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruijiacao/Documents/Ruijia/Research/eliciting_beliefs_llm/eliciting_beliefs_ai/.conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadModel:\n",
    "    def __init__(self, model_name: str, layer_idx: int, token_pos: int, device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name, output_hidden_states=True).to(device)\n",
    "        self.model.eval()\n",
    "        self.layer_idx = layer_idx\n",
    "        self.token_pos = token_pos\n",
    "        self.device = device\n",
    "\n",
    "    def extract_activation(self, text: str):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        \n",
    "        hidden_states = outputs.hidden_states  # tuple: (embeddings, layer1, layer2, ..., final layer)\n",
    "\n",
    "        if self.layer_idx >= len(hidden_states):\n",
    "            raise ValueError(f\"Layer index {self.layer_idx} is out of range.\")\n",
    "        \n",
    "        activation = hidden_states[self.layer_idx]  # (batch_size, seq_len, hidden_dim)\n",
    "        \n",
    "        # Often we take the activation at the final token\n",
    "        activation = activation[:, self.token_pos, :]  # shape: (batch_size, hidden_dim)\n",
    "\n",
    "        return activation.squeeze(0).cpu()  # Return a 1D numpy array\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPProbe(nn.Module):\n",
    "    \"\"\"\n",
    "    This a two-layer MLP for a potential non-linear probe\n",
    "    \"\"\"\n",
    "    def __init__(self, d):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d, 100)\n",
    "        self.linear2 = nn.Linear(100, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.linear1(x))\n",
    "        o = self.linear2(h)\n",
    "        return torch.sigmoid(o)\n",
    "\n",
    "class CCS(object):\n",
    "    def __init__(self, x0, x1, nepochs=1000, ntries=10, lr=1e-3, batch_size=-1, \n",
    "                 verbose=False, device=\"cpu\", linear=True, weight_decay=0.01, var_normalize=False):\n",
    "        # data\n",
    "        self.var_normalize = var_normalize\n",
    "        self.x0 = self.normalize(x0)\n",
    "        self.x1 = self.normalize(x1)\n",
    "        self.d = self.x0.shape[-1]\n",
    "\n",
    "        # training\n",
    "        self.nepochs = nepochs\n",
    "        self.ntries = ntries\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        # probe\n",
    "        self.linear = linear\n",
    "        self.initialize_probe()\n",
    "        self.best_probe = copy.deepcopy(self.probe)\n",
    "\n",
    "        \n",
    "    def initialize_probe(self):\n",
    "        if self.linear:\n",
    "            self.probe = nn.Sequential(nn.Linear(self.d, 1), nn.Sigmoid())\n",
    "        else:\n",
    "            self.probe = MLPProbe(self.d)\n",
    "        self.probe.to(self.device)    \n",
    "\n",
    "\n",
    "    def normalize(self, x):\n",
    "        \"\"\"\n",
    "        Mean-normalizes the data x (of shape (n, d))\n",
    "        If self.var_normalize, also divides by the standard deviation\n",
    "        \"\"\"\n",
    "        normalized_x = x - x.mean(axis=0, keepdims=True)\n",
    "        if self.var_normalize:\n",
    "            normalized_x /= normalized_x.std(axis=0, keepdims=True)\n",
    "\n",
    "        return normalized_x\n",
    "\n",
    "        \n",
    "    def get_tensor_data(self):\n",
    "        \"\"\"\n",
    "        Returns x0, x1 as appropriate tensors (rather than np arrays)\n",
    "        \"\"\"\n",
    "        x0 = torch.tensor(self.x0, dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        x1 = torch.tensor(self.x1, dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        return x0, x1\n",
    "    \n",
    "\n",
    "    def get_loss(self, p0, p1):\n",
    "        \"\"\"\n",
    "        Returns the CCS loss for two probabilities each of shape (n,1) or (n,)\n",
    "        \"\"\"\n",
    "        informative_loss = (torch.min(p0, p1)**2).mean(0)\n",
    "        consistent_loss = ((p0 - (1-p1))**2).mean(0)\n",
    "        return informative_loss + consistent_loss\n",
    "\n",
    "\n",
    "    def get_acc(self, x0_test, x1_test, y_test):\n",
    "        \"\"\"\n",
    "        Computes accuracy for the current parameters on the given test inputs\n",
    "        \"\"\"\n",
    "        x0 = torch.tensor(self.normalize(x0_test), dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        x1 = torch.tensor(self.normalize(x1_test), dtype=torch.float, requires_grad=False, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            p0, p1 = self.best_probe(x0), self.best_probe(x1)\n",
    "        avg_confidence = 0.5*(p0 + (1-p1))\n",
    "        predictions = (avg_confidence.detach().cpu().numpy() < 0.5).astype(int)[:, 0]\n",
    "        acc = (predictions == y_test).mean()\n",
    "        acc = max(acc, 1 - acc)\n",
    "\n",
    "        return acc\n",
    "    \n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Does a single training run of nepochs epochs\n",
    "        \"\"\"\n",
    "        x0, x1 = self.get_tensor_data()\n",
    "        permutation = torch.randperm(len(x0))\n",
    "        x0, x1 = x0[permutation], x1[permutation]\n",
    "        \n",
    "        # set up optimizer\n",
    "        optimizer = torch.optim.AdamW(self.probe.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        \n",
    "        batch_size = len(x0) if self.batch_size == -1 else self.batch_size\n",
    "        nbatches = len(x0) // batch_size\n",
    "\n",
    "        # Start training (full batch)\n",
    "        for epoch in range(self.nepochs):\n",
    "            for j in range(nbatches):\n",
    "                x0_batch = x0[j*batch_size:(j+1)*batch_size]\n",
    "                x1_batch = x1[j*batch_size:(j+1)*batch_size]\n",
    "            \n",
    "                # probe\n",
    "                p0, p1 = self.probe(x0_batch), self.probe(x1_batch)\n",
    "\n",
    "                # get the corresponding loss\n",
    "                loss = self.get_loss(p0, p1)\n",
    "\n",
    "                # update the parameters\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        return loss.detach().cpu().item()\n",
    "    \n",
    "    def repeated_train(self):\n",
    "        best_loss = np.inf\n",
    "        for train_num in range(self.ntries):\n",
    "            self.initialize_probe()\n",
    "            loss = self.train()\n",
    "            if loss < best_loss:\n",
    "                self.best_probe = copy.deepcopy(self.probe)\n",
    "                best_loss = loss\n",
    "\n",
    "        return best_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"  # Replace with your model\n",
    "layer_idx = -2  # Example layer\n",
    "token_pos = -1\n",
    "\n",
    "trainer = LoadModel(model_name, layer_idx, token_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_texts = [\"Captial of France is Paris\"]\n",
    "negative_texts = [\"Captial of France is not Paris\"]\n",
    "positive_tests = [\n",
    "    \"Captial of China is not Beijing\"\n",
    "    \"Captial of USA is Washington D.C..\"\n",
    "]\n",
    "\n",
    "negative_tests = [\n",
    "    \"Captial of China is Beijing\",\n",
    "    \"Captial of USA is not Washington D.C..\"\n",
    "]\n",
    "pos_hs_train = torch.stack([trainer.extract_activation(text) for text in positive_texts]) \n",
    "neg_hs_train = torch.stack([trainer.extract_activation(text) for text in negative_texts])\n",
    "\n",
    "pos_hs_test = torch.stack([trainer.extract_activation(text) for text in positive_tests]) \n",
    "neg_hs_test = torch.stack([trainer.extract_activation(text) for text in negative_tests])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/wyw5y57d1pg63jf5nqv55_lc0000gq/T/ipykernel_19718/3652077333.py:60: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x0 = torch.tensor(self.x0, dtype=torch.float, requires_grad=False, device=self.device)\n",
      "/var/folders/pc/wyw5y57d1pg63jf5nqv55_lc0000gq/T/ipykernel_19718/3652077333.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x1 = torch.tensor(self.x1, dtype=torch.float, requires_grad=False, device=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.type of Sequential(\n",
      "  (0): Linear(in_features=2048, out_features=1, bias=True)\n",
      "  (1): Sigmoid()\n",
      ")>\n",
      "CCS accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/wyw5y57d1pg63jf5nqv55_lc0000gq/T/ipykernel_19718/3652077333.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x0 = torch.tensor(self.normalize(x0_test), dtype=torch.float, requires_grad=False, device=self.device)\n",
      "/var/folders/pc/wyw5y57d1pg63jf5nqv55_lc0000gq/T/ipykernel_19718/3652077333.py:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x1 = torch.tensor(self.normalize(x1_test), dtype=torch.float, requires_grad=False, device=self.device)\n"
     ]
    }
   ],
   "source": [
    "# Train CCS without any labels\n",
    "ccs = CCS(neg_hs_train, pos_hs_train)\n",
    "ccs.repeated_train()\n",
    "probe = ccs.best_probe\n",
    "# print(probe.type)\n",
    "y_test = [0, 1]\n",
    "\n",
    "# Evaluate\n",
    "ccs_acc = ccs.get_acc(neg_hs_test, pos_hs_test, y_test)\n",
    "print(\"CCS accuracy: {}\".format(ccs_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CCS on cities.csv\n",
    "#set the directory to the location of your data\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>label</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>correct_country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The city of Krasnodar is in Russia.</td>\n",
       "      <td>1</td>\n",
       "      <td>Krasnodar</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The city of Krasnodar is in South Africa.</td>\n",
       "      <td>0</td>\n",
       "      <td>Krasnodar</td>\n",
       "      <td>South Africa</td>\n",
       "      <td>Russia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The city of Lodz is in Poland.</td>\n",
       "      <td>1</td>\n",
       "      <td>Lodz</td>\n",
       "      <td>Poland</td>\n",
       "      <td>Poland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The city of Lodz is in the Dominican Republic.</td>\n",
       "      <td>0</td>\n",
       "      <td>Lodz</td>\n",
       "      <td>the Dominican Republic</td>\n",
       "      <td>Poland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The city of Maracay is in Venezuela.</td>\n",
       "      <td>1</td>\n",
       "      <td>Maracay</td>\n",
       "      <td>Venezuela</td>\n",
       "      <td>Venezuela</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        statement  label       city  \\\n",
       "0             The city of Krasnodar is in Russia.      1  Krasnodar   \n",
       "1       The city of Krasnodar is in South Africa.      0  Krasnodar   \n",
       "2                  The city of Lodz is in Poland.      1       Lodz   \n",
       "3  The city of Lodz is in the Dominican Republic.      0       Lodz   \n",
       "4            The city of Maracay is in Venezuela.      1    Maracay   \n",
       "\n",
       "                  country correct_country  \n",
       "0                  Russia          Russia  \n",
       "1            South Africa          Russia  \n",
       "2                  Poland          Poland  \n",
       "3  the Dominican Republic          Poland  \n",
       "4               Venezuela       Venezuela  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath = \"../datasets\"\n",
    "df_cities = pd.read_csv(os.path.join(datapath, \"cities.csv\"))\n",
    "df_cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive examples: 748\n",
      "Negative examples: 748\n"
     ]
    }
   ],
   "source": [
    "# Split into positive and negative statement lists\n",
    "positive_statements = df_cities[df_cities['label'] == 1]['statement'].tolist()\n",
    "negative_statements = df_cities[df_cities['label'] == 0]['statement'].tolist()\n",
    "\n",
    "print(f\"Positive examples: {len(positive_statements)}\")\n",
    "print(f\"Negative examples: {len(negative_statements)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Positive Activations:   0%|          | 0/748 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Extracting Positive Activations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 748/748 [07:36<00:00,  1.64it/s]\n",
      "Extracting Negative Activations: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 748/748 [07:39<00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive activations shape: torch.Size([748, 2048])\n",
      "Negative activations shape: torch.Size([748, 2048])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "#  Extract hidden states for positives and negatives with a progress bar\n",
    "pos_hs_cities = torch.stack([\n",
    "    trainer.extract_activation(text) for text in tqdm(positive_statements, desc=\"Extracting Positive Activations\")\n",
    "])\n",
    "\n",
    "neg_hs_cities = torch.stack([\n",
    "    trainer.extract_activation(text) for text in tqdm(negative_statements, desc=\"Extracting Negative Activations\")\n",
    "])\n",
    "\n",
    "print(f\"Positive activations shape: {pos_hs_cities.shape}\")\n",
    "print(f\"Negative activations shape: {neg_hs_cities.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize(x0: np.ndarray, x1: np.ndarray):\n",
    "    \"\"\"Shuffle positives and negatives together, then split.\"\"\"\n",
    "    X = np.vstack([x0, x1])\n",
    "    y = np.array([1]*len(x0) + [0]*len(x1))\n",
    "    perm = np.random.permutation(len(X))\n",
    "    X, y = X[perm], y[perm]\n",
    "    return X[y == 1], X[y == 0]\n",
    "\n",
    "\n",
    "DATA_PATH = \"../datasets/cities.csv\"   # adjust as needed\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "positive_statements = df[df.label == 1][\"statement\"].tolist()\n",
    "negative_statements = df[df.label == 0][\"statement\"].tolist()\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "token_pos = -1  \n",
    "\n",
    "\n",
    "tmp = AutoModelForCausalLM.from_pretrained(model_name, output_hidden_states=True)\n",
    "num_layers = tmp.config.num_hidden_layers\n",
    "del tmp\n",
    "\n",
    "\n",
    "\n",
    "results = {\"layer\": [], \"ccs_accuracy\": [], \"lr_accuracy\": []}\n",
    "\n",
    "for layer_idx in range(num_layers):\n",
    "    print(f\"\\nðŸ”µ Layer {layer_idx}/{num_layers-1}\")\n",
    "\n",
    "    #  extract activations at this layer\n",
    "    loader = LoadModel(model_name, layer_idx, token_pos, device)\n",
    "    pos_hs = torch.stack([\n",
    "        loader.extract_activation(t) for t in tqdm(positive_statements, desc=\"Positive activations\")\n",
    "    ]).numpy()\n",
    "    neg_hs = torch.stack([\n",
    "        loader.extract_activation(t) for t in tqdm(negative_statements, desc=\"Negative activations\")\n",
    "    ]).numpy()\n",
    "\n",
    "    #  randomize\n",
    "    x0_r, x1_r = randomize(pos_hs, neg_hs)\n",
    "\n",
    "    # train CCS\n",
    "    ccs = CCS(x0_r, x1_r, nepochs=1000, ntries=5, lr=1e-3, device=device)\n",
    "    ccs.repeated_train()\n",
    "    y_dummy = np.zeros(len(x0_r))\n",
    "    acc_ccs = ccs.get_acc(x0_r, x1_r, y_dummy)\n",
    "\n",
    "    #  train Logistic Regression\n",
    "    X_lr = np.vstack([x0_r, x1_r])\n",
    "    y_lr = np.array([1]*len(x0_r) + [0]*len(x1_r))\n",
    "    clf = LogisticRegression(class_weight=\"balanced\", max_iter=1000)\n",
    "    clf.fit(X_lr, y_lr)\n",
    "    acc_lr = clf.score(X_lr, y_lr)\n",
    "\n",
    "    # record\n",
    "    results[\"layer\"].append(layer_idx)\n",
    "    results[\"ccs_accuracy\"].append(acc_ccs)\n",
    "    results[\"lr_accuracy\"].append(acc_lr)\n",
    "\n",
    "    print(f\"CCS acc: {acc_ccs:.4f} | LR acc: {acc_lr:.4f}\")\n",
    "\n",
    "\n",
    "#  Save results DataFrame\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nðŸ“ Final Results:\")\n",
    "print(results_df)\n",
    "\n",
    "results_df.to_csv(\"ccs_lr_layerwise_results.csv\", index=False)\n",
    "print(\"\\n Results saved to ccs_lr_layerwise_results.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
