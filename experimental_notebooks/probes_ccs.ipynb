{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# Training Probes using Contrast-Consistent Search (CCS)\n",
       "\n",
       "This notebook demonstrates how to train probes using the CCS approach from Burns et al. (2023). CCS is a method for learning probes that can extract beliefs from language models without using labeled data.\n",
       "\n",
       "The key idea is to train a probe that:\n",
       "1. Makes informative predictions (assigns high probabilities to at least one of the contrast pairs)\n",
       "2. Makes consistent predictions (assigns complementary probabilities to contrast pairs)\n",
       "\n",
       "For example, if we have a statement \"The sky is blue\", we create two contrast pairs:\n",
       "- \"The sky is blue\" (positive)\n",
       "- \"The sky is not blue\" (negative)\n",
       "\n",
       "The probe should assign high probability to one of these and low probability to the other."
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "%load_ext autoreload\n",
       "%autoreload 2\n",
       "\n",
       "import torch\n",
       "import torch.nn as nn\n",
       "import torch.nn.functional as F\n",
       "import numpy as np\n",
       "import copy\n",
       "from sklearn.linear_model import LogisticRegression\n",
       "import matplotlib.pyplot as plt\n",
       "import itertools"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "class MLPProbe(nn.Module):\n",
       "    def __init__(self, d):\n",
       "        super().__init__()\n",
       "        self.linear1 = nn.Linear(d, 100)\n",
       "        self.linear2 = nn.Linear(100, 1)\n",
       "\n",
       "    def forward(self, x):\n",
       "        h = F.relu(self.linear1(x))\n",
       "        o = self.linear2(h)\n",
       "        return torch.sigmoid(o)\n",
       "\n",
       "class CCS(object):\n",
       "    def __init__(self, x0, x1, nepochs=1000, ntries=10, lr=1e-3, batch_size=-1, \n",
       "                 verbose=False, device=\"cpu\", linear=True, weight_decay=0.01, var_normalize=False):\n",
       "        # data\n",
       "        self.var_normalize = var_normalize\n",
       "        self.x0 = self.normalize(x0)\n",
       "        self.x1 = self.normalize(x1)\n",
       "        self.d = self.x0.shape[-1]\n",
       "\n",
       "        # training\n",
       "        self.nepochs = nepochs\n",
       "        self.ntries = ntries\n",
       "        self.lr = lr\n",
       "        self.verbose = verbose\n",
       "        self.device = device\n",
       "        self.batch_size = batch_size\n",
       "        self.weight_decay = weight_decay\n",
       "        \n",
       "        # probe\n",
       "        self.linear = linear\n",
       "        self.initialize_probe()\n",
       "        self.best_probe = copy.deepcopy(self.probe)\n",
       "\n",
       "    def initialize_probe(self):\n",
       "        if self.linear:\n",
       "            self.probe = nn.Sequential(nn.Linear(self.d, 1), nn.Sigmoid())\n",
       "        else:\n",
       "            self.probe = MLPProbe(self.d)\n",
       "        self.probe.to(self.device)    \n",
       "\n",
       "    def normalize(self, x):\n",
       "        \"\"\"\n",
       "        Mean-normalizes the data x (of shape (n, d))\n",
       "        If self.var_normalize, also divides by the standard deviation\n",
       "        \"\"\"\n",
       "        normalized_x = x - x.mean(axis=0, keepdims=True)\n",
       "        if self.var_normalize:\n",
       "            normalized_x /= normalized_x.std(axis=0, keepdims=True)\n",
       "        return normalized_x\n",
       "\n",
       "    def get_tensor_data(self):\n",
       "        \"\"\"\n",
       "        Returns x0, x1 as appropriate tensors (rather than np arrays)\n",
       "        \"\"\"\n",
       "        x0 = torch.tensor(self.x0, dtype=torch.float, requires_grad=False, device=self.device)\n",
       "        x1 = torch.tensor(self.x1, dtype=torch.float, requires_grad=False, device=self.device)\n",
       "        return x0, x1\n",
       "    \n",
       "    def get_loss(self, p0, p1):\n",
       "        \"\"\"\n",
       "        Returns the CCS loss for two probabilities each of shape (n,1) or (n,)\n",
       "        \"\"\"\n",
       "        informative_loss = (torch.min(p0, p1)**2).mean(0)\n",
       "        consistent_loss = ((p0 - (1-p1))**2).mean(0)\n",
       "        return informative_loss + consistent_loss\n",
       "\n",
       "    def get_acc(self, x0_test, x1_test, y_test):\n",
       "        \"\"\"\n",
       "        Computes accuracy for the current parameters on the given test inputs\n",
       "        \"\"\"\n",
       "        x0 = torch.tensor(self.normalize(x0_test), dtype=torch.float, requires_grad=False, device=self.device)\n",
       "        x1 = torch.tensor(self.normalize(x1_test), dtype=torch.float, requires_grad=False, device=self.device)\n",
       "        with torch.no_grad():\n",
       "            p0, p1 = self.best_probe(x0), self.best_probe(x1)\n",
       "        avg_confidence = 0.5*(p0 + (1-p1))\n",
       "        predictions = (avg_confidence.detach().cpu().numpy() < 0.5).astype(int)[:, 0]\n",
       "        acc = (predictions == y_test).mean()\n",
       "        acc = max(acc, 1 - acc)\n",
       "        return acc\n",
       "    \n",
       "    def train(self):\n",
       "        \"\"\"\n",
       "        Does a single training run of nepochs epochs\n",
       "        \"\"\"\n",
       "        x0, x1 = self.get_tensor_data()\n",
       "        permutation = torch.randperm(len(x0))\n",
       "        x0, x1 = x0[permutation], x1[permutation]\n",
       "        \n",
       "        # set up optimizer\n",
       "        optimizer = torch.optim.AdamW(self.probe.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
       "        \n",
       "        batch_size = len(x0) if self.batch_size == -1 else self.batch_size\n",
       "        nbatches = len(x0) // batch_size\n",
       "\n",
       "        # Start training (full batch)\n",
       "        for epoch in range(self.nepochs):\n",
       "            for j in range(nbatches):\n",
       "                x0_batch = x0[j*batch_size:(j+1)*batch_size]\n",
       "                x1_batch = x1[j*batch_size:(j+1)*batch_size]\n",
       "            \n",
       "                # probe\n",
       "                p0, p1 = self.probe(x0_batch), self.probe(x1_batch)\n",
       "\n",
       "                # get the corresponding loss\n",
       "                loss = self.get_loss(p0, p1)\n",
       "\n",
       "                # update the parameters\n",
       "                optimizer.zero_grad()\n",
       "                loss.backward()\n",
       "                optimizer.step()\n",
       "\n",
       "        return loss.detach().cpu().item()\n",
       "    \n",
       "    def repeated_train(self):\n",
       "        best_loss = np.inf\n",
       "        for train_num in range(self.ntries):\n",
       "            self.initialize_probe()\n",
       "            loss = self.train()\n",
       "            if loss < best_loss:\n",
       "                self.best_probe = copy.deepcopy(self.probe)\n",
       "                best_loss = loss\n",
       "        return best_loss"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "def get_hidden_states_many_examples(model, tokenizer, data, dataset_name, model_type, params):\n",
       "    \"\"\"\n",
       "    Given an encoder-decoder model, a list of data, computes the contrast hidden states on n random examples by probing the model according to the specified parameters (param = (layer_indices, token_positions, prompt_versions))\n",
       "    Returns numpy arrays of shape (n, hidden_dim) for each candidate label, along with a boolean numpy array of shape (n,)\n",
       "    with the ground truth labels\n",
       "    \n",
       "    This is deliberately simple so that it's easy to understand, rather than being optimized for efficiency\n",
       "    \"\"\"\n",
       "    # setup\n",
       "    model.eval()\n",
       "    all_neg_hs, all_pos_hs, all_gt_labels = [], [], []\n",
       "    layer, token_pos, prompt_version = params \n",
       "\n",
       "    # loop\n",
       "    for sample in data:\n",
       "        if dataset_name == \"imdb\":\n",
       "            text, true_label = sample[\"text\"], sample[\"label\"]\n",
       "            neg_hs = utils.get_hidden_states(model, tokenizer, format_imdb(text, 0, prompt_version), token_pos, layer, model_type=model_type)\n",
       "            pos_hs = utils.get_hidden_states(model, tokenizer, format_imdb(text, 1, prompt_version), token_pos, layer, model_type=model_type)\n",
       "        elif dataset_name == \"google/boolq\":\n",
       "            text, question, true_label = sample[\"passage\"], sample[\"question\"], sample[\"answer\"]\n",
       "            neg_hs = utils.get_hidden_states(model, tokenizer, format_boolq(text, question, 0), token_pos, layer, model_type=model_type)\n",
       "            pos_hs = utils.get_hidden_states(model, tokenizer, format_boolq(text, question, 1), token_pos, layer, model_type=model_type)\n",
       "        elif dataset_name == \"domenicrosati/TruthfulQA\":\n",
       "            question, best_answer, incorrect_answer = sample[\"question\"], sample[\"best answer\"], sample[\"incorrect answer\"]\n",
       "            neg_hs = utils.get_hidden_states(model, tokenizer, format_truthfulqa(question, best_answer, 0), token_pos, layer, model_type=model_type) \n",
       "            pos_hs = utils.get_hidden_states(model, tokenizer, format_truthfulqa(question, best_answer, 1), token_pos, layer, model_type=model_type)\n",
       "\n",
       "            neg_hs_2 = utils.get_hidden_states(model, tokenizer, format_truthfulqa(question, incorrect_answer, 0), token_pos, layer, model_type=model_type)\n",
       "            pos_hs_2 = utils.get_hidden_states(model, tokenizer, format_truthfulqa(question, incorrect_answer, 1), token_pos, layer, model_type=model_type)\n",
       "            all_neg_hs.append(neg_hs_2)\n",
       "            all_pos_hs.append(pos_hs_2)\n",
       "            \n",
       "        # collect\n",
       "        all_neg_hs.append(neg_hs)\n",
       "        all_pos_hs.append(pos_hs)\n",
       "        all_gt_labels.append(true_label)\n",
       "\n",
       "    all_neg_hs = np.stack(all_neg_hs)\n",
       "    all_pos_hs = np.stack(all_pos_hs)\n",
       "    all_gt_labels = np.stack(all_gt_labels)\n",
       "\n",
       "    return all_neg_hs, all_pos_hs, all_gt_labels"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Example usage\n",
       "\n",
       "# Hyperparameters\n",
       "num_example = 100\n",
       "layer_indices = [-1]  # Use last layer\n",
       "token_positions = [-1]  # Use last token\n",
       "prompt_versions = [1]\n",
       "\n",
       "# Get hidden states for training and testing\n",
       "neg_hs, pos_hs, y = get_hidden_states_many_examples(\n",
       "    model, \n",
       "    tokenizer, \n",
       "    data_train[\"imdb\"][:num_example], \n",
       "    \"imdb\", \n",
       "    model_type=\"encoder_decoder\",\n",
       "    params=(layer_indices[0], token_positions[0], prompt_versions[0])\n",
       ")\n",
       "\n",
       "# Split into train/test\n",
       "n = len(y)\n",
       "neg_hs_train, neg_hs_test = neg_hs[:n//2], neg_hs[n//2:]\n",
       "pos_hs_train, pos_hs_test = pos_hs[:n//2], pos_hs[n//2:]\n",
       "y_train, y_test = y[:n//2], y[n//2:]\n",
       "\n",
       "# Verify with logistic regression first\n",
       "x_train = neg_hs_train - pos_hs_train\n",
       "x_test = neg_hs_test - pos_hs_test\n",
       "lr = LogisticRegression(class_weight=\"balanced\")\n",
       "lr.fit(x_train, y_train)\n",
       "print(\"Logistic regression accuracy: {}\".format(lr.score(x_test, y_test)))\n",
       "\n",
       "# Train CCS probe\n",
       "ccs = CCS(\n",
       "    neg_hs_train,\n",
       "    pos_hs_train,\n",
       "    nepochs=1000,\n",
       "    ntries=10,\n",
       "    lr=1e-3,\n",
       "    verbose=True,\n",
       "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
       "    linear=True,\n",
       "    weight_decay=0.01,\n",
       "    var_normalize=False\n",
       ")\n",
       "\n",
       "# Train and evaluate\n",
       "best_loss = ccs.repeated_train()\n",
       "ccs_acc = ccs.get_acc(neg_hs_test, pos_hs_test, y_test)\n",
       "\n",
       "print(f\"Best CCS loss: {best_loss:.4f}\")\n",
       "print(f\"CCS accuracy: {ccs_acc:.4f}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Function to aggregate credences using geometric mean\n",
       "def aggregate_gmean(credences):\n",
       "    \"\"\"Aggregate a list of credences into one estimate using geometric mean.\"\"\"\n",
       "    k = np.shape(credences)[0]\n",
       "    result = np.power(np.prod(credences * (1 / (1 - credences))), 1 / k)\n",
       "    return 1 / (1 + result)\n",
       "\n",
       "# Get predictions and visualize\n",
       "with torch.no_grad():\n",
       "    pos_probs = ccs.best_probe(torch.tensor(pos_hs_test, dtype=torch.float32, device=ccs.device))\n",
       "    neg_probs = ccs.best_probe(torch.tensor(neg_hs_test, dtype=torch.float32, device=ccs.device))\n",
       "\n",
       "# Average confidence\n",
       "avg_confidence = 0.5 * (pos_probs.cpu() + (1 - neg_probs.cpu()))\n",
       "\n",
       "# Visualize distribution of confidences\n",
       "plt.figure(figsize=(10, 6))\n",
       "plt.hist(avg_confidence.numpy(), bins=20, alpha=0.7)\n",
       "plt.axvline(x=0.5, color='r', linestyle='--', label='Decision Boundary')\n",
       "plt.xlabel('Confidence')\n",
       "plt.ylabel('Count')\n",
       "plt.title('Distribution of CCS Probe Confidences')\n",
       "plt.legend()\n",
       "plt.show()"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 2
   }